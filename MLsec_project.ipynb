{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelboi/MLsec_project/blob/main/MLsec_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models:\n",
        "1. Fixing Data Augmentation to Improve Adversarial Robustness\n",
        "2. Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?\n",
        "3. MMA Training: Direct Input Space Margin Maximization through Adversarial Training"
      ],
      "metadata": {
        "id": "CcZOaHCtHRqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General imports"
      ],
      "metadata": {
        "id": "CHr8lWVj_FFu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzCcoczZsmbn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Various\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import importlib.util\n",
        "import re\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD, Optimizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# RobustBench\n",
        "!pip install git+https://github.com/manuelboi/robustbench\n",
        "from robustbench.utils import load_model\n",
        "from robustbench.eval import benchmark\n",
        "from robustbench.data import load_cifar10\n",
        "from robustbench.model_zoo.enums import ThreatModel\n",
        "\n",
        "# Smoothing\n",
        "!git clone https://github.com/matteoturnu/smoothing.git\n",
        "!conda create -n smoothing\n",
        "!conda activate smoothing\n",
        "!conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n",
        "!conda install scipy pandas statsmodels matplotlib seaborn\n",
        "!pip install setGPU\n",
        "from smoothing.code.core import Smooth\n",
        "from smoothing.code.train import train, test\n",
        "from smoothing.code.train_utils import AverageMeter, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "bRZkGSpiPFiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# Set labels\n",
        "labels_dct = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"}\n",
        "\n",
        "# Use GPU if available, otherwise use CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Computing device used: \", device)\n",
        "\n",
        "# Preparing trainset and testset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Set trainset and testset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the number of samples you want to take from the whole dataset\n",
        "num_samples = 1000\n",
        "\n",
        "# Create a subset of the dataset with the chosen number of samples\n",
        "indices = list(range(num_samples))\n",
        "train_subset = Subset(trainset, indices)\n",
        "test_subset = Subset(testset, indices)\n",
        "\n",
        "# Preparing trainloader and testloader\n",
        "batch_size = 64\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load test samples for predict and certify\n",
        "n_test_samples_benchmark = 100\n",
        "x_test, y_test = load_cifar10(n_test_samples_benchmark)\n",
        "x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "\n",
        "# Setting various parameters\n",
        "epochs = 3 # used for training\n",
        "sigma = 0.25 # gaussian noise standard deviation\n",
        "n_examples = 20 # to be perturbed by AutoAttack\n",
        "eps_L2 = 0.5 # epsilon of the perturbancy for L2 norm\n",
        "eps_Linf = 8/255 # epsilon of the perturbancy with Linf norm\n",
        "version = 'custom'\n",
        "attacks_to_run=['apgd-ce', 'apgd-dlr'] # Attacks to run on the models"
      ],
      "metadata": {
        "id": "ur-xfxzJPEo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "QwmN0U3EC3To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to compact donwload and configuration of the model"
      ],
      "metadata": {
        "id": "eWnSWe-pEldG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_configure_model(name, dataset, threat_model, device):\n",
        "  # Download model from robustbench\n",
        "  model = load_model(model_name=name, dataset=dataset, threat_model=threat_model)\n",
        "\n",
        "  # Set model to the chosen device\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Set model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "WfH7lE2U_b71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for smooth model training"
      ],
      "metadata": {
        "id": "Symqohu2KxSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(trainloader, testloader, model, epochs, sigma, device):\n",
        "  criterion = CrossEntropyLoss().to(device)\n",
        "  optimizer = SGD(model.parameters())\n",
        "  scheduler = StepLR(optimizer, step_size=30)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    scheduler.step(epoch)\n",
        "    before = time.time()\n",
        "    train_loss, train_acc = train(trainloader, model, criterion, optimizer, epoch, sigma, device)\n",
        "    test_loss, test_acc = test(testloader, model, criterion, epoch, sigma, device)\n",
        "    after = time.time()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "cntdRl_hD0TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for measuring performances on smooth model"
      ],
      "metadata": {
        "id": "1w6B16vREnO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def certify(model, sigma, x_text, y_test, L):\n",
        "  n_classes = 10\n",
        "  alpha = 1 # (1 - alpha) is the confidence level (in this case is 0%)\n",
        "  n0 = 10 # number of samples for selection\n",
        "  n = 20 # number of samples for estimation (certify) (too few samples but computation time is strongly affected with more)\n",
        "\n",
        "  smooth_model = Smooth(model, n_classes, sigma)\n",
        "\n",
        "  top_classes = list()\n",
        "  radiuses = list()\n",
        "  for x, y in zip(x_test, y_test):\n",
        "    top_class = smooth_model.predict(x, n0, alpha, batch_size=n0, device=device,)\n",
        "    top_class, radius = smooth_model.certify(x, n0, n, alpha, batch_size=n0, device=device, L=L)\n",
        "    top_classes.append(top_class)\n",
        "    radiuses.append(radius)\n",
        "\n",
        "  top_classes = torch.tensor(top_classes, dtype=torch.float64).to(device)\n",
        "  accuracy = torch.mean(top_classes == y_test, dtype=torch.float64)\n",
        "  print(\"Radiuses list: \", radiuses)\n",
        "  radius = torch.mean(torch.Tensor(radiuses), dtype=torch.float64)\n",
        "\n",
        "\n",
        "  print(\"Top classes: \", top_classes)\n",
        "  print(\"Y classes: \", y_test)\n",
        "  print(f\"Average radious found by certify: {float(radius.item())}\")\n",
        "  percentage = float(accuracy.item()) * 100\n",
        "  print(f\"Accuracy found by certify function: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "tGu5x_t8EabV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to compact test of the model with certification and AutoAttack"
      ],
      "metadata": {
        "id": "NqyswXEcFB_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, sigma, x_test, y_test, norm, threat_model, n_examples, eps, batch_size, device, version, attacks_to_run):\n",
        "  # Model prediction and certification on smoothed samples\n",
        "  certify(model, sigma, x_test, y_test, norm)\n",
        "\n",
        "  # AutoAttack on given model\n",
        "  benchmark(model,\n",
        "            threat_model=threat_model,\n",
        "            n_examples=n_examples,\n",
        "            eps=eps,\n",
        "            batch_size=batch_size,\n",
        "            device=device,\n",
        "            version=version,\n",
        "            attacks_to_run=attacks_to_run)"
      ],
      "metadata": {
        "id": "MW9m9mnr3jlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixing Data Augmentation to Improve Adversarial Robustness (WideResNet-70-16)"
      ],
      "metadata": {
        "id": "RmnI2rBDeOEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "AvnvKSRLBp9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_1_L2 = load_configure_model(name='Rebuffi2021Fixing_70_16_cutmix_extra', dataset='cifar10', threat_model='L2', device=device)\n",
        "\n",
        "# Test on stock model\n",
        "test_model(model_1_L2, sigma, x_test, y_test, 'L2', ThreatModel.L2, n_examples, eps_L2, batch_size, device, version, attacks_to_run)\n",
        "\n",
        "# Smoothed model training\n",
        "smoothed_model_1_L2 = train_model(trainloader, testloader, model_1_L2, epochs, sigma, device)\n",
        "\n",
        "# Test on smoothed model\n",
        "test_model(smoothed_model_1_L2, sigma, x_test, y_test, 'L2', ThreatModel.L2, n_examples, eps_L2, batch_size, device, version, attacks_to_run)"
      ],
      "metadata": {
        "id": "TbLV6iYN_UuY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "n6gPMiYKByXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_1_Linf = load_configure_model(name='Rebuffi2021Fixing_70_16_cutmix_extra', dataset='cifar10', threat_model='Linf', device=device)\n",
        "\n",
        "# Test on stock model\n",
        "test_model(model_1_Linf, sigma, x_test, y_test, 'Linf', ThreatModel.Linf, n_examples, eps_Linf, batch_size, device, version, attacks_to_run)\n",
        "\n",
        "# Smoothed model training\n",
        "smoothed_model_1_Linf = train_model(trainloader, testloader, model_1_Linf, epochs, sigma, device)\n",
        "\n",
        "# Test on smoothed model\n",
        "test_model(smoothed_model_1_Linf, sigma, x_test, y_test, 'Linf', ThreatModel.Linf, n_examples, eps_Linf, batch_size, device, version, attacks_to_run)"
      ],
      "metadata": {
        "id": "rA09MQ0lZ34c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness? (ResNet-18)"
      ],
      "metadata": {
        "id": "mtma9ehWeKjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "qSW7z1l1Iten"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_2_L2 = load_configure_model(name='Sehwag2021Proxy_R18', dataset='cifar10', threat_model='L2', device=device)\n",
        "\n",
        "# Test on stock model\n",
        "test_model(model_2_L2, sigma, x_test, y_test, 'L2', ThreatModel.L2, n_examples, eps_L2, batch_size, device, version, attacks_to_run)\n",
        "\n",
        "# Smoothed model training\n",
        "smoothed_model_2_L2 = train_model(trainloader, testloader, model_2_L2, epochs, sigma, device)\n",
        "\n",
        "# Test on smoothed model\n",
        "test_model(smoothed_model_2_L2, sigma, x_test, y_test, 'L2', ThreatModel.L2, n_examples, eps_L2, batch_size, device, version, attacks_to_run)"
      ],
      "metadata": {
        "id": "hOl_1rinc4IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "fajFJ2DaI5x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_2_Linf = load_configure_model(name='Sehwag2021Proxy_R18', dataset='cifar10', threat_model='L2', device=device)\n",
        "\n",
        "# Test on stock model\n",
        "test_model(model_2_Linf, sigma, x_test, y_test, 'Linf', ThreatModel.Linf, n_examples, eps_Linf, batch_size, device, version, attacks_to_run)\n",
        "\n",
        "# Smoothed model training\n",
        "smoothed_model_2_Linf = train_model(trainloader, testloader, model_2_Linf, epochs, sigma, device)\n",
        "\n",
        "# Test on smoothed model\n",
        "test_model(smoothed_model_2_Linf, sigma, x_test, y_test, 'Linf', ThreatModel.Linf, n_examples, eps_Linf, batch_size, device, version, attacks_to_run)"
      ],
      "metadata": {
        "id": "mAv_TuSXtC9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMA Training: Direct Input Space Margin Maximization through Adversarial Training (WideResNet-28-4)"
      ],
      "metadata": {
        "id": "I2nOuLxdeSz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "R-6Nq9NPJ_Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_3_L2 = load_configure_model(name='Ding2020MMA', dataset='cifar10', threat_model='L2', device=device)\n",
        "\n",
        "# Test on stock model\n",
        "test_model(model_3_L2, sigma, x_test, y_test, 'L2', ThreatModel.L2, n_examples, eps_L2, batch_size, device, version, attacks_to_run)\n",
        "\n",
        "# Smoothed model training\n",
        "smoothed_model_3_L2 = train_model(trainloader, testloader, model_3_L2, epochs, sigma, device)\n",
        "\n",
        "# Test on smoothed model\n",
        "test_model(smoothed_model_3_L2, sigma, x_test, y_test, 'L2', ThreatModel.L2, n_examples, eps_L2, batch_size, device, version, attacks_to_run)"
      ],
      "metadata": {
        "id": "SA8WQsoZeWIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "n4dNHMZQKP9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_3_Linf = load_configure_model(name='Ding2020MMA', dataset='cifar10', threat_model='Linf', device=device)\n",
        "\n",
        "# Test on stock model\n",
        "test_model(model_3_Linf, sigma, x_test, y_test, 'Linf', ThreatModel.Linf, n_examples, eps_Linf, batch_size, device, version, attacks_to_run)\n",
        "\n",
        "# Smoothed model training\n",
        "smoothed_model_3_Linf = train_model(trainloader, testloader, model_3_Linf, epochs, sigma, device)\n",
        "\n",
        "# Test on smoothed model\n",
        "test_model(smoothed_model_3_Linf, sigma, x_test, y_test, 'Linf', ThreatModel.Linf, n_examples, eps_Linf, batch_size, device, version, attacks_to_run)"
      ],
      "metadata": {
        "id": "DbXlugPutzr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}