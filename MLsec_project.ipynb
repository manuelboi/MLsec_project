{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelboi/MLsec_project/blob/main/MLsec_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models:\n",
        "1. Fixing Data Augmentation to Improve Adversarial Robustness\n",
        "2. Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?\n",
        "3. MMA Training: Direct Input Space Margin Maximization through Adversarial Training"
      ],
      "metadata": {
        "id": "CcZOaHCtHRqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General imports"
      ],
      "metadata": {
        "id": "CHr8lWVj_FFu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CzCcoczZsmbn",
        "collapsed": true,
        "outputId": "c1568bd9-cea2-4e3e-c9ba-3f5ef1f71b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 102 (core.py, line 103)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-d85ba1decc63>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0;36m, in \u001b[0;35m<cell line: 32>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from smoothing.code.core import Smooth\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/content/smoothing/code/core.py\"\u001b[0;36m, line \u001b[0;32m103\u001b[0m\n\u001b[0;31m    \"\"\" Sample the base classifier's prediction under noisy corruptions of the input x using Linf noise.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 102\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "\n",
        "# Various\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import importlib.util\n",
        "import re\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD, Optimizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# RobustBench\n",
        "!pip install git+https://github.com/RobustBench/robustbench\n",
        "from robustbench.utils import load_model\n",
        "from robustbench.eval import benchmark\n",
        "from robustbench.data import load_cifar10\n",
        "from robustbench.model_zoo.enums import ThreatModel\n",
        "\n",
        "# Smoothing\n",
        "!git clone https://github.com/matteoturnu/smoothing.git\n",
        "# smoothing repository dependencies\n",
        "!conda create -n smoothing\n",
        "!conda activate smoothing\n",
        "!conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n",
        "!conda install scipy pandas statsmodels matplotlib seaborn\n",
        "!pip install setGPU\n",
        "from smoothing.code.core import Smooth\n",
        "from smoothing.code.train import train, test\n",
        "from smoothing.code.train_utils import AverageMeter, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "bRZkGSpiPFiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dct = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"}\n",
        "# use GPU if available, otherwise use CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Computing device used: \", device)\n",
        "\n",
        "# Preparing trainset and testset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 64\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define the number of samples you want to take\n",
        "num_samples = 100\n",
        "# Create a subset of the dataset\n",
        "indices = list(range(num_samples))\n",
        "train_subset = Subset(trainset, indices)\n",
        "test_subset = Subset(testset, indices)\n",
        "\n",
        "# Preparing trainloader and testloader\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load test samples for predict and certify\n",
        "x_test, y_test = load_cifar10(10)\n",
        "x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "\n",
        "# Setting various parameters\n",
        "epochs = 1 # used for training\n",
        "sigma = 0.5 # gaussian noise standard deviation\n",
        "n_examples = 5 # to be perturbed by AutoAttack\n",
        "eps_L2 = 0.5 # epsilon of the perturbancy for L2 norm\n",
        "eps_Linf = 8/255 # epsilon of the perturbancy with Linf norm"
      ],
      "metadata": {
        "id": "ur-xfxzJPEo3",
        "outputId": "9918c373-4299-41c6-a107-4da39dc6e365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing device used:  cpu\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 59107897.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "QwmN0U3EC3To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training"
      ],
      "metadata": {
        "id": "Symqohu2KxSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(trainloader, testloader, model, epochs, sigma, device):\n",
        "  criterion = CrossEntropyLoss().to(device)\n",
        "  optimizer = SGD(model.parameters())\n",
        "  scheduler = StepLR(optimizer, step_size=30)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    scheduler.step(epoch)\n",
        "    before = time.time()\n",
        "    train_loss, train_acc = train(trainloader, model, criterion, optimizer, epoch, sigma, device)\n",
        "    test_loss, test_acc = test(testloader, model, criterion, epoch, sigma, device)\n",
        "    after = time.time()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "cntdRl_hD0TB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smooth model performances on perturbed images"
      ],
      "metadata": {
        "id": "1w6B16vREnO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def certify(model, sigma, x_text, y_test):\n",
        "  n_classes = 10\n",
        "  alpha = 0.05 # (1 - alpha) is the confidence level (in this case is 0.95)\n",
        "  n0 = 10 # number of samples for selection\n",
        "  n = 100 # number of samples for estimation (certify) (too few samples but computation time is strongly affected with more)\n",
        "\n",
        "  smooth_model = Smooth(model, n_classes, sigma)\n",
        "\n",
        "  top_classes = list()\n",
        "  radiuses = list()\n",
        "  for x, y in zip(x_test, y_test):\n",
        "    top_class = smooth_model.predict(x, n0, alpha, batch_size=n0, device=device)\n",
        "    top_class, radius = smooth_model.certify(x, n0, n, alpha, batch_size=n0, device=device)\n",
        "    top_classes.append(top_class)\n",
        "    radiuses.append(radius)\n",
        "\n",
        "  top_classes = torch.tensor(top_classes, dtype=torch.float64)\n",
        "  accuracy = torch.mean(top_classes == y_test, dtype=torch.float64)\n",
        "\n",
        "  print(\"Top classes: \", top_classes)\n",
        "  print(\"Y classes: \", y_test)\n",
        "  print(\"Accuracy on smooth model: \", accuracy)"
      ],
      "metadata": {
        "id": "tGu5x_t8EabV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixing Data Augmentation to Improve Adversarial Robustness"
      ],
      "metadata": {
        "id": "RmnI2rBDeOEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "AvnvKSRLBp9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_1_L2 = load_model(model_name='Rebuffi2021Fixing_70_16_cutmix_extra', dataset='cifar10', threat_model='L2')\n",
        "model_1_L2.to(device)\n",
        "\n",
        "# Model training\n",
        "model_1_L2 = train_model(trainloader, testloader, model_1_L2, epochs, sigma, device, 'L2')\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_1_L2, sigma, x_test, y_test)\n",
        "\n",
        "# AutoAttack on model 1 with L2\n",
        "benchmark(model_1_L2, threat_model=ThreatModel.L2, n_examples=n_examples, eps=eps_L2)"
      ],
      "metadata": {
        "id": "TbLV6iYN_UuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "n6gPMiYKByXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_1_Linf = load_model(model_name='Rebuffi2021Fixing_70_16_cutmix_extra', dataset='cifar10', threat_model='Linf')\n",
        "model_1_Linf.to(device)\n",
        "\n",
        "# Model training\n",
        "model_1_Linf = train_model(trainloader, testloader, model_1_Linf, epochs, sigma, device, 'Linf')\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_1_Linf, sigma, x_test, y_test)\n",
        "\n",
        "# AutoAttack on model 1 with Linf\n",
        "benchmark(model_1_Linf, threat_model=ThreatModel.Linf, n_examples=n_examples, eps=eps_Linf)"
      ],
      "metadata": {
        "id": "rA09MQ0lZ34c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?"
      ],
      "metadata": {
        "id": "mtma9ehWeKjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "qSW7z1l1Iten"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_2_L2 = load_model(model_name='Sehwag2021Proxy', dataset='cifar10', threat_model='L2')\n",
        "model_2_L2.to(device)\n",
        "\n",
        "# Model training\n",
        "model_2_L2 = train_model(trainloader, testloader, model_2_L2, epochs, sigma, device, 'L2')\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_2_L2, sigma, x_test, y_test)\n",
        "\n",
        "# AutoAttack on model 2 with L2\n",
        "benchmark(model_2_L2, threat_model=ThreatModel.L2, n_examples=n_examples, eps=eps_L2)"
      ],
      "metadata": {
        "id": "hOl_1rinc4IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "fajFJ2DaI5x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_2_Linf = load_model(model_name='Sehwag2021Proxy', dataset='cifar10', threat_model='Linf')\n",
        "model_2_Linf.to(device)\n",
        "\n",
        "# Model training\n",
        "model_2_Linf = train_model(trainloader, testloader, model_2_Linf, epochs, sigma, device, 'Linf')\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_2_Linf, sigma, x_test, y_test)\n",
        "\n",
        "# AutoAttack on model 2 with Linf\n",
        "benchmark(model_2_Linf, threat_model=ThreatModel.Linf, n_examples=n_examples, eps=eps_Linf)"
      ],
      "metadata": {
        "id": "mAv_TuSXtC9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMA Training: Direct Input Space Margin Maximization through Adversarial Training"
      ],
      "metadata": {
        "id": "I2nOuLxdeSz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "R-6Nq9NPJ_Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_3_L2 = load_model(model_name='Ding2020MMA', dataset='cifar10', threat_model='L2')\n",
        "model_3_L2.to(device)\n",
        "\n",
        "# Model training\n",
        "model_3_L2 = train_model(trainloader, testloader, model_3_L2, epochs, sigma, device, 'L2')\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_3_L2, sigma, x_test, y_test)\n",
        "\n",
        "# AutoAttack on model 3 with L2\n",
        "benchmark(model_3_L2, threat_model=ThreatModel.L2, n_examples=n_examples, eps=eps_L2)"
      ],
      "metadata": {
        "id": "SA8WQsoZeWIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "n4dNHMZQKP9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_3_Linf = load_model(model_name='Sehwag2021Proxy', dataset='cifar10', threat_model='Linf')\n",
        "model_3_Linf.to(device)\n",
        "\n",
        "# Model training\n",
        "model_3_Linf = train_model(trainloader, testloader, model_3_Linf, epochs, sigma, device, 'Linf')\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_3_Linf, sigma, x_test, y_test)\n",
        "\n",
        "# AutoAttack on model 3 with Linf\n",
        "benchmark(model_3_Linf, threat_model=ThreatModel.Linf, n_examples=n_examples, eps=eps_Linf)"
      ],
      "metadata": {
        "id": "DbXlugPutzr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}