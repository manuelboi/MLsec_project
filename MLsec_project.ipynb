{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manuelboi/MLsec_project/blob/main/MLsec_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models:\n",
        "1. Fixing Data Augmentation to Improve Adversarial Robustness\n",
        "2. Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?\n",
        "3. MMA Training: Direct Input Space Margin Maximization through Adversarial Training"
      ],
      "metadata": {
        "id": "CcZOaHCtHRqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General imports"
      ],
      "metadata": {
        "id": "CHr8lWVj_FFu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CzCcoczZsmbn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Various\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import importlib.util\n",
        "import re\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import SGD, Optimizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# RobustBench\n",
        "!pip install git+https://github.com/RobustBench/robustbench\n",
        "from robustbench.utils import load_model\n",
        "from robustbench.eval import benchmark\n",
        "from robustbench.data import load_cifar10\n",
        "from robustbench.model_zoo.enums import ThreatModel\n",
        "\n",
        "# Smoothing\n",
        "!git clone https://github.com/matteoturnu/smoothing.git\n",
        "!conda create -n smoothing\n",
        "!conda activate smoothing\n",
        "!conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n",
        "!conda install scipy pandas statsmodels matplotlib seaborn\n",
        "!pip install setGPU\n",
        "from smoothing.code.core import Smooth\n",
        "from smoothing.code.train import train, test\n",
        "from smoothing.code.train_utils import AverageMeter, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "bRZkGSpiPFiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "labels_dct = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"}\n",
        "# use GPU if available, otherwise use CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Computing device used: \", device)\n",
        "\n",
        "# Preparing trainset and testset\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 64\n",
        "n_test_samples_benchmark = 100\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "\"\"\"\n",
        "# Define the number of samples you want to take\n",
        "num_samples = 100\n",
        "# Create a subset of the dataset\n",
        "indices = list(range(num_samples))\n",
        "train_subset = Subset(trainset, indices)\n",
        "test_subset = Subset(testset, indices)\n",
        "\n",
        "# Preparing trainloader and testloader\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\"\"\"\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load test samples for predict and certify\n",
        "x_test, y_test = load_cifar10(n_test_samples_benchmark)\n",
        "x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "\n",
        "# Setting various parameters\n",
        "epochs = 3 # used for training\n",
        "sigma = 0.5 # gaussian noise standard deviation\n",
        "n_examples = 5 # to be perturbed by AutoAttack\n",
        "eps_L2 = 0.5 # epsilon of the perturbancy for L2 norm\n",
        "eps_Linf = 8/255 # epsilon of the perturbancy with Linf norm"
      ],
      "metadata": {
        "id": "ur-xfxzJPEo3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "QwmN0U3EC3To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training"
      ],
      "metadata": {
        "id": "Symqohu2KxSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(trainloader, testloader, model, epochs, sigma, device):\n",
        "  criterion = CrossEntropyLoss().to(device)\n",
        "  optimizer = SGD(model.parameters())\n",
        "  scheduler = StepLR(optimizer, step_size=30)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    scheduler.step(epoch)\n",
        "    before = time.time()\n",
        "    train_loss, train_acc = train(trainloader, model, criterion, optimizer, epoch, sigma, device)\n",
        "    test_loss, test_acc = test(testloader, model, criterion, epoch, sigma, device)\n",
        "    after = time.time()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "cntdRl_hD0TB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smooth model performances on perturbed images"
      ],
      "metadata": {
        "id": "1w6B16vREnO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def certify(model, sigma, x_text, y_test, L):\n",
        "  n_classes = 10\n",
        "  alpha = 0.05 # (1 - alpha) is the confidence level (in this case is 0.95)\n",
        "  n0 = 10 # number of samples for selection\n",
        "  n = 20 # number of samples for estimation (certify) (too few samples but computation time is strongly affected with more)\n",
        "\n",
        "  smooth_model = Smooth(model, n_classes, sigma)\n",
        "\n",
        "  top_classes = list()\n",
        "  radiuses = list()\n",
        "  for x, y in zip(x_test, y_test):\n",
        "    top_class = smooth_model.predict(x, n0, alpha, batch_size=n0, device=device,)\n",
        "    top_class, radius = smooth_model.certify(x, n0, n, alpha, batch_size=n0, device=device, L=L)\n",
        "    top_classes.append(top_class)\n",
        "    radiuses.append(radius)\n",
        "\n",
        "  top_classes = torch.tensor(top_classes, dtype=torch.float64).to(device)\n",
        "  accuracy = torch.mean(top_classes == y_test, dtype=torch.float64)\n",
        "\n",
        "  print(\"Top classes: \", top_classes)\n",
        "  print(\"Y classes: \", y_test)\n",
        "  print(\"Accuracy on smooth model: \", accuracy)"
      ],
      "metadata": {
        "id": "tGu5x_t8EabV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixing Data Augmentation to Improve Adversarial Robustness"
      ],
      "metadata": {
        "id": "RmnI2rBDeOEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "AvnvKSRLBp9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_1_L2 = load_model(model_name='Rebuffi2021Fixing_70_16_cutmix_extra', dataset='cifar10', threat_model='L2')\n",
        "model_1_L2 = model_1_L2.to(device)\n",
        "\n",
        "# Model training\n",
        "model_1_L2 = train_model(trainloader, testloader, model_1_L2, epochs, sigma, device)\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_1_L2, sigma, x_test, y_test, 'L2')\n",
        "\n",
        "# AutoAttack on model 1 with L2\n",
        "benchmark(model_1_L2, threat_model=ThreatModel.L2, n_examples=n_examples, eps=eps_L2, batch_size=batch_size, device=device)"
      ],
      "metadata": {
        "id": "TbLV6iYN_UuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94c08c3b-cdeb-4fa1-8596-19a190c3fa32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "n6gPMiYKByXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_1_Linf = load_model(model_name='Rebuffi2021Fixing_70_16_cutmix_extra', dataset='cifar10', threat_model='Linf')\n",
        "model_1_Linf = model_1_Linf.to(device)\n",
        "\n",
        "# Model training\n",
        "model_1_Linf = train_model(trainloader, testloader, model_1_Linf, epochs, sigma, device)\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_1_Linf, sigma, x_test, y_test, 'Linf')\n",
        "\n",
        "# AutoAttack on model 1 with Linf\n",
        "benchmark(model_1_Linf, threat_model=ThreatModel.Linf, n_examples=n_examples, eps=eps_Linf, batch_size=batch_size, device=device)"
      ],
      "metadata": {
        "id": "rA09MQ0lZ34c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?"
      ],
      "metadata": {
        "id": "mtma9ehWeKjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "qSW7z1l1Iten"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_2_L2 = load_model(model_name='Sehwag2021Proxy', dataset='cifar10', threat_model='L2')\n",
        "model_2_L2 = model_2_L2.to(device)\n",
        "\n",
        "# Model training\n",
        "model_2_L2 = train_model(trainloader, testloader, model_2_L2, epochs, sigma, device)\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_2_L2, sigma, x_test, y_test, 'L2')\n",
        "\n",
        "# AutoAttack on model 2 with L2\n",
        "benchmark(model_2_L2, threat_model=ThreatModel.L2, n_examples=n_examples, eps=eps_L2, batch_size=batch_size, device=device)"
      ],
      "metadata": {
        "id": "hOl_1rinc4IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "fajFJ2DaI5x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_2_Linf = load_model(model_name='Sehwag2021Proxy', dataset='cifar10', threat_model='Linf')\n",
        "model_2_Linf = model_2_Linf.to(device)\n",
        "\n",
        "# Model training\n",
        "model_2_Linf = train_model(trainloader, testloader, model_2_Linf, epochs, sigma, device)\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_2_Linf, sigma, x_test, y_test, 'Linf')\n",
        "\n",
        "# AutoAttack on model 2 with Linf\n",
        "benchmark(model_2_Linf, threat_model=ThreatModel.Linf, n_examples=n_examples, eps=eps_Linf, batch_size=batch_size, device=device)"
      ],
      "metadata": {
        "id": "mAv_TuSXtC9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMA Training: Direct Input Space Margin Maximization through Adversarial Training"
      ],
      "metadata": {
        "id": "I2nOuLxdeSz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L2"
      ],
      "metadata": {
        "id": "R-6Nq9NPJ_Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_3_L2 = load_model(model_name='Ding2020MMA', dataset='cifar10', threat_model='L2')\n",
        "model_3_L2 = model_3_L2.to(device)\n",
        "\n",
        "# Model training\n",
        "model_3_L2 = train_model(trainloader, testloader, model_3_L2, epochs, sigma, device)\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_3_L2, sigma, x_test, y_test, 'L2')\n",
        "\n",
        "# AutoAttack on model 3 with L2\n",
        "benchmark(model_3_L2, threat_model=ThreatModel.L2, n_examples=n_examples, eps=eps_L2, batch_size=batch_size, device=device)"
      ],
      "metadata": {
        "id": "SA8WQsoZeWIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linf"
      ],
      "metadata": {
        "id": "n4dNHMZQKP9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model loading\n",
        "model_3_Linf = load_model(model_name='Sehwag2021Proxy', dataset='cifar10', threat_model='Linf')\n",
        "model_3_Linf = model_3_Linf.to(device)\n",
        "\n",
        "# Model training\n",
        "model_3_Linf = train_model(trainloader, testloader, model_3_Linf, epochs, sigma, device)\n",
        "\n",
        "# Model prediction and certification\n",
        "certify(model_3_Linf, sigma, x_test, y_test, 'Linf')\n",
        "\n",
        "# AutoAttack on model 3 with Linf\n",
        "benchmark(model_3_Linf, threat_model=ThreatModel.Linf, n_examples=n_examples, eps=eps_Linf, batch_size=batch_size, device=device)"
      ],
      "metadata": {
        "id": "DbXlugPutzr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}